\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{tabularx}


\input{../../tex/preamble.tex}
\renewcommand{\cource}{Параллельная обработка данных}

\begin{document}
\makemytitlepage{1}{Message Passing Interface (MPI)}

\se{Цель работы}

Знакомство с технологией MPI. Реализация метода Якоби.
Решение задачи Дирихле для уравнения Лапласа в трехмерной области с граничными
условиями первого рода.

\textbf{Вариант 8.} обмен граничными слоями через isend/irecv, контроль сходимости allreduce

\textbf{Входные данные.}

На первой строке заданы три числа: размер сетки
процессов. Гарантируется, что при запуске программы количество процессов будет
равно произведению этих трех чисел. На второй строке задается размер блока,
который будет обрабатываться одним процессом: три числа. Далее задается путь к
выходному файлу, в который необходимо записать конечный результат работы
программы и точность $\varepsilon$ . На последующих строках описывается задача: задаются
размеры области $l_x$, $l_y$ и $l_z$,
граничные условия: $u_down$, $u_up$, $u_left$, $u_right$, $u_front$ и $u_back$,
и начальное значение $u^0$.

\textbf{Выходные данные.}

В файл, определенный во входных данных, необходимо
напечатать построчно значения $(u_{1,1,1}, u_{2,1,1}, \cdots, u_{2,2,1}, \cdots, u_{n_x,n_y,n_z})$
в ячейках сетки в формате с плавающей запятой с семью знаками мантиссы.

\nvidia

\se{Метод решения}

Корневой процесс считывает условие задачи и при помощи \lstinline{MPI_Bcast} передаёт их
остальным процессам.

Вычисления проходят итеративно и состоят из следующих шагов:

\begin{enumerate}
	\item Обмен граничными слоями.
	\item Пересчёт.
	\item Вычисление погрешности.
\end{enumerate}


\subparagraph*{Обмен граничными слоями.}

Так как вычисления происходят в разных процессах (возможно и на разных машинах),
то им нужно обмениваться граничными значениями для вычисления следующей итерации.

Для каждой их трёх координат происходит одни и те же операции для верхней и нижней границы.
Каждый процесс передаёт свои граничные условия смежному процессу.

Данные с границы записываются в отдельный буфер, который отправляется/принимается посредством \lstinline{MPI_ISend/MPI_IRecv}.

Отличие этих функций от \lstinline{MPI_Send/MPI_Recv} в том, что они не блокирующие.
Так в случае использования \lstinline{MPI_Send/MPI_Recv} пришлось бы явно разделить условия:

\begin{lstlisting}[language=c++]
if (current_rank == upper_process) {
    MPI_Send(...);
} else {
    MPI_Recv(...);
}

if (current_rank == upper_process) {
    MPI_Recv(...);
} else {
    MPI_Send(...);
}
\end{lstlisting}

А при использовании \lstinline{MPI_ISend/MPI_IRecv} достаточно дождаться завершения обоих запросов.
\begin{lstlisting}[language=c++]
MPI_request req[2];
MPI_Recv(..., &req[0]);
MPI_Send(..., &req[1]);
MPI_Waitall(2, req, ...);
\end{lstlisting}

От себя добавлю, что уже есть функция \lstinline{MPI_SendRecv}, которая делает то же самое, но в одну строчку.

\subparagraph*{Пересчёт.}

После обмена граничными условиями происходят основные вычисления.

Для варианта с CUDA было нужно аккуратно использовать индексацию, чтобы доступ к памяти был последовательным.

\begin{lstlisting}[language=c++,basicstyle=\scriptsize]
__global__ void compute_kernel(
    double* out, double* data,
    BlockGrid grid,
    mydim3<double> h)
{
    const int id_x = threadIdx.x + blockIdx.x * blockDim.x,
              id_y = threadIdx.y + blockIdx.y * blockDim.y,
              id_z = threadIdx.z + blockIdx.z * blockDim.z,
              offset_x = blockDim.x * gridDim.x,
              offset_y = blockDim.y * gridDim.y,
              offset_z = blockDim.z * gridDim.z;

    const double inv_hx = 1.0 / (h.x * h.x),
                 inv_hy = 1.0 / (h.y * h.y),
                 inv_hz = 1.0 / (h.z * h.z);

    for (int i = id_x; i < grid.bsize.x; i += offset_x) {
        for (int j = id_y; j < grid.bsize.y; j += offset_y) {
            for (int k = id_z; k < grid.bsize.z; k += offset_z) {
                double num = 0.0
                    + (data[grid.cell_absolute_id(i + 1, j, k)] + data[grid.cell_absolute_id(i - 1, j, k)]) * inv_hx
                    + (data[grid.cell_absolute_id(i, j + 1, k)] + data[grid.cell_absolute_id(i, j - 1, k)]) * inv_hy
                    + (data[grid.cell_absolute_id(i, j, k + 1)] + data[grid.cell_absolute_id(i, j, k - 1)]) * inv_hz;
                double denum = 2.0 * (inv_hx + inv_hy + inv_hz);

                out[grid.cell_absolute_id(i, j, k)] = num / denum;
            }
        }
    }
}


__global__ void abs_error_kernel(double* out, double* data, BlockGrid grid)
{
    const int idx = threadIdx.x + blockIdx.x * blockDim.x,
              idy = threadIdx.y + blockIdx.y * blockDim.y,
              idz = threadIdx.z + blockIdx.z * blockDim.z,
              offset_x = blockDim.x * gridDim.x,
              offset_y = blockDim.y * gridDim.y,
              offset_z = blockDim.z * gridDim.z;

    for (int i = idx - 1; i <= grid.bsize.x; i += offset_x) {
        for (int j = idy - 1; j <= grid.bsize.y; j += offset_y) {
            for (int k = idz - 1; k <= grid.bsize.z; k += offset_z) {
                int cell_id = grid.cell_absolute_id(i, j, k);
                if (i == -1 || j == -1 || k == -1
                    || i == grid.bsize.x || j == grid.bsize.y || k == grid.bsize.z) {
                    out[cell_id] = 0.0;
                } else {
                    out[cell_id] = fabsf(out[cell_id] - data[cell_id]);
                }
            }
        }
    }
}

double DeviceKernels::compute(double* out, double* data, mydim3<double> height)
{
    START_KERNEL((
        compute_kernel<<<BORDER_DIMS_3D(kernel_block_dim, kernel_grid_dim)>>>(
            out, data, grid, height)));

    START_KERNEL((
        abs_error_kernel<<<BORDER_DIMS_3D(kernel_block_dim, kernel_grid_dim)>>>(
            data, out, grid)));

    thrust::device_ptr<double> dev_ptr = thrust::device_pointer_cast(data);
    double error = *thrust::max_element(dev_ptr, dev_ptr + grid.cells_per_block());

    CUDA_ERR(cudaGetLastError());
    return error;
}
\end{lstlisting}

Для лабораторной с openmp я попытался создать сетку потоков ``a-la cuda'' и варп из нескольких процессов.

\begin{lstlisting}

\end{lstlisting}

\subparagraph*{Вычисление погрешности.}

С погрешностью всё довольно весело из за того что вычисления разделены на несколько процессов.

И огромное счастье, что в MPI есть способ применить простую функцию к данным с нескольких процессов.

\begin{lstlisting}[language=c++]
double local_error = calc(...);
double all_error;
MPI_ERR(MPI_Allreduce(&local_error, &all_error, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD));
\end{lstlisting}

Всем процессам вернётся максимальное число из ошибок всех процессов.

Так можно определить условие окончания вычислений - достижение заданной погрешности.

% \se{Исходный код}

% \listsource{../src}{main.cpp}
% \listsource{../src}{solver.hpp}
% \listsource{../src}{solver.cpp}
% \listsource{../src}{common.cpp}
% \listsource{../src}{exchange.hpp}
% \listsource{../src}{exchange.cpp}
% \listsource{../src}{problem.hpp}
% \listsource{../src}{problem.cpp}
% \listsource{../src}{grid/grid.hpp}
% \listsource{../src}{grid/grid.cpp}
% \listsource{../src}{dim3/dim3.hpp}
% \listsource{../src}{dim3/dim3.cpp}

\se{Результаты}
\noindent
\begin{table*}[!htb]
	\begin{minipage}{.49\linewidth}
		\centering
		\caption*{Область 8x8x8}
		точность $1e^{-10}$
		\begin{tabularx}{\linewidth}{|X|X|}
			\hline
			n processes & time      \\
			\hline
			1           & 21.607302 \\
			2           & 18.843787 \\
			4           & 14.497888 \\
			8           & 35.551439 \\
			16          & 98.548130 \\
			\hline
		\end{tabularx}
	\end{minipage}%
	\begin{minipage}{.49\linewidth}
		\centering
		\caption*{Область 64x64x64}
		точность $1e^{-2}$
		\begin{tabularx}{\linewidth}{|X|X|}
			\hline
			n processes & time        \\
			\hline
			1           & 6968.731496 \\
			2           & 4522.690277 \\
			4           & 2542.007506 \\
			8           & 2081.170354 \\
			16          & 795.745062  \\
			32          & 616.174570  \\
			\hline
		\end{tabularx}
	\end{minipage}

	\begin{minipage}{.49\linewidth}
		\centering
		\caption*{Область 128x128x128}
		точность $1e^{-2}$
		\begin{tabularx}{\linewidth}{|X|X|}
			\hline
			n processes & time        \\
			\hline
			1           & 45277053369 \\
			2           & 49619365214 \\
			4           & 20064615792 \\
			8           & 8796538972 \\
			16          &  \\
			32          &  \\
			\hline
		\end{tabularx}
	\end{minipage}%
\end{table*}

\se{Выводы}

\end{document}