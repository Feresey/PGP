\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{tabularx}


\input{../../tex/preamble.tex}
\renewcommand{\cource}{Параллельная обработка данных}

\begin{document}
\makemytitlepage{1}{Message Passing Interface (MPI)}

\se{Цель работы}

Знакомство с технологией MPI. Реализация метода Якоби.
Решение задачи Дирихле для уравнения Лапласа в трехмерной области с граничными
условиями первого рода.

\textbf{Вариант 8.} обмен граничными слоями через isend/irecv, контроль сходимости allreduce

\textbf{Входные данные.}

На первой строке заданы три числа: размер сетки
процессов. Гарантируется, что при запуске программы количество процессов будет
равно произведению этих трех чисел. На второй строке задается размер блока,
который будет обрабатываться одним процессом: три числа. Далее задается путь к
выходному файлу, в который необходимо записать конечный результат работы
программы и точность $\varepsilon$ . На последующих строках описывается задача: задаются
размеры области $l_x$, $l_y$ и $l_z$,
граничные условия: $u_down$, $u_up$, $u_left$, $u_right$, $u_front$ и $u_back$,
и начальное значение $u^0$.

\textbf{Выходные данные.}

В файл, определенный во входных данных, необходимо
напечатать построчно значения $(u_{1,1,1}, u_{2,1,1}, \cdots, u_{2,2,1}, \cdots, u_{n_x,n_y,n_z})$
в ячейках сетки в формате с плавающей запятой с семью знаками мантиссы.

\nvidia

\se{Метод решения}

Корневой процесс считывает условие задачи и при помощи \lstinline{MPI_Bcast} передаёт их
остальным процессам.

Вычисления проходят итеративно и состоят из следующих шагов:

\begin{enumerate}
	\item Обмен граничными слоями.
	\item Пересчёт.
	\item Вычисление погрешности.
\end{enumerate}


\subparagraph*{Обмен граничными слоями.}

Так как вычисления происходят в разных процессах (возможно и на разных машинах),
то им нужно обмениваться граничными значениями для вычисления следующей итерации.

Для каждой их трёх координат происходит одни и те же операции для верхней и нижней границы.
Каждый процесс передаёт свои граничные условия смежному процессу.

Данные с границы записываются в отдельный буфер, который отправляется/принимается посредством \lstinline{MPI_ISend/MPI_IRecv}.

Отличие этих функций от \lstinline{MPI_Send/MPI_Recv} в том, что они не блокирующие.
Так в случае использования \lstinline{MPI_Send/MPI_Recv} пришлось бы явно разделить условия:

\begin{lstlisting}[language=c++]
if (current_rank == upper_process) {
    MPI_Send(...);
} else {
    MPI_Recv(...);
}

if (current_rank == upper_process) {
    MPI_Recv(...);
} else {
    MPI_Send(...);
}
\end{lstlisting}

А при использовании \lstinline{MPI_ISend/MPI_IRecv} достаточно дождаться завершения обоих запросов.
\begin{lstlisting}[language=c++]
MPI_request req[2];
MPI_Recv(..., &req[0]);
MPI_Send(..., &req[1]);
MPI_Waitall(2, req, ...);
\end{lstlisting}

От себя добавлю, что уже есть функция \lstinline{MPI_SendRecv}, которая делает то же самое, но в одну строчку.

\subparagraph*{Пересчёт.}

После обмена граничными условиями происходят основные вычисления.

Для варианта с CUDA было нужно аккуратно использовать индексацию, чтобы доступ к памяти был последовательным.

\begin{lstlisting}[language=c++,basicstyle=\scriptsize]
__global__ void compute_kernel(
    double* out, double* data,
    BlockGrid grid,
    mydim3<double> h)
{
    const int id_x = threadIdx.x + blockIdx.x * blockDim.x,
              id_y = threadIdx.y + blockIdx.y * blockDim.y,
              id_z = threadIdx.z + blockIdx.z * blockDim.z,
              offset_x = blockDim.x * gridDim.x,
              offset_y = blockDim.y * gridDim.y,
              offset_z = blockDim.z * gridDim.z;

    const double inv_hx = 1.0 / (h.x * h.x),
                 inv_hy = 1.0 / (h.y * h.y),
                 inv_hz = 1.0 / (h.z * h.z);

    for (int i = id_x; i < grid.bsize.x; i += offset_x) {
        for (int j = id_y; j < grid.bsize.y; j += offset_y) {
            for (int k = id_z; k < grid.bsize.z; k += offset_z) {
                double num = 0.0
                    + (data[grid.cell_absolute_id(i + 1, j, k)] + data[grid.cell_absolute_id(i - 1, j, k)]) * inv_hx
                    + (data[grid.cell_absolute_id(i, j + 1, k)] + data[grid.cell_absolute_id(i, j - 1, k)]) * inv_hy
                    + (data[grid.cell_absolute_id(i, j, k + 1)] + data[grid.cell_absolute_id(i, j, k - 1)]) * inv_hz;
                double denum = 2.0 * (inv_hx + inv_hy + inv_hz);

                out[grid.cell_absolute_id(i, j, k)] = num / denum;
            }
        }
    }
}


__global__ void abs_error_kernel(double* out, double* data, BlockGrid grid)
{
    const int idx = threadIdx.x + blockIdx.x * blockDim.x,
              idy = threadIdx.y + blockIdx.y * blockDim.y,
              idz = threadIdx.z + blockIdx.z * blockDim.z,
              offset_x = blockDim.x * gridDim.x,
              offset_y = blockDim.y * gridDim.y,
              offset_z = blockDim.z * gridDim.z;

    for (int i = idx - 1; i <= grid.bsize.x; i += offset_x) {
        for (int j = idy - 1; j <= grid.bsize.y; j += offset_y) {
            for (int k = idz - 1; k <= grid.bsize.z; k += offset_z) {
                int cell_id = grid.cell_absolute_id(i, j, k);
                if (i == -1 || j == -1 || k == -1
                    || i == grid.bsize.x || j == grid.bsize.y || k == grid.bsize.z) {
                    out[cell_id] = 0.0;
                } else {
                    out[cell_id] = fabsf(out[cell_id] - data[cell_id]);
                }
            }
        }
    }
}

double DeviceKernels::compute(double* out, double* data, mydim3<double> height)
{
    START_KERNEL((
        compute_kernel<<<BORDER_DIMS_3D(kernel_block_dim, kernel_grid_dim)>>>(
            out, data, grid, height)));

    START_KERNEL((
        abs_error_kernel<<<BORDER_DIMS_3D(kernel_block_dim, kernel_grid_dim)>>>(
            data, out, grid)));

    thrust::device_ptr<double> dev_ptr = thrust::device_pointer_cast(data);
    double error = *thrust::max_element(dev_ptr, dev_ptr + grid.cells_per_block());

    CUDA_ERR(cudaGetLastError());
    return error;
}
\end{lstlisting}

Для лабораторной с openmp я попытался создать сетку потоков ``a-la cuda'' и варп из нескольких процессов.

\begin{lstlisting}[language=c++,basicstyle=\scriptsize]
double Problem::calc()
{
    double max_error = 0.0;

    const dim3<double> inv_h = {
        1.0 / (height.x * height.x),
        1.0 / (height.y * height.y),
        1.0 / (height.z * height.z)
    };

    auto& data = this->data;

#pragma omp parallel shared(data) reduction(max:max_error)
    {
        int n_threads = omp_get_num_threads();
        int rank = omp_get_thread_num();

        for (int x = 0; x < grid.bsize.x; x += n_threads) {
            for (int y = 0; y < grid.bsize.y; y += n_threads) {
                for (int z = 0; z < grid.bsize.z; z += n_threads) {
                    double error = 0.0;

                    for (int bx = 0; bx < n_threads; ++bx) {
                        for (int by = 0; by < n_threads; ++by) {
                            int i = x + bx, j = y + by, k = z + rank;
                            if (i >= grid.bsize.x || j >= grid.bsize.y || k >= grid.bsize.z) {
                                continue;
                            }
                            double num = 0.0
                                + (data[grid.cell_absolute_id(i + 1, j, k)] + data[grid.cell_absolute_id(i - 1, j, k)]) * inv_h.x
                                + (data[grid.cell_absolute_id(i, j + 1, k)] + data[grid.cell_absolute_id(i, j - 1, k)]) * inv_h.y
                                + (data[grid.cell_absolute_id(i, j, k + 1)] + data[grid.cell_absolute_id(i, j, k - 1)]) * inv_h.z;

                            double denum = 2.0 * (inv_h.x + inv_h.y + inv_h.z);
                            double temp = num / denum;
                            error = std::fabs(data[grid.cell_absolute_id(i, j, k)] - temp);

                            if (error > max_error) {
                                max_error = error;
                            }

                            data_next[grid.cell_absolute_id(i, j, k)] = temp;
                        }
                    }
                }
            }
        }
    }

    std::swap(this->data, this->data_next);
    return max_error;
}
\end{lstlisting}

\subparagraph*{Вычисление погрешности.}

С погрешностью всё довольно весело из за того что вычисления разделены на несколько процессов.

И огромное счастье, что в MPI есть способ применить простую функцию к данным с нескольких процессов.

\begin{lstlisting}[language=c++]
double local_error = calc(...);
double all_error;
MPI_ERR(MPI_Allreduce(&local_error, &all_error, 1,
                      MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD));
\end{lstlisting}

Всем процессам вернётся максимальное число из ошибок всех процессов.

Так можно определить условие окончания вычислений - достижение заданной погрешности.

% \se{Исходный код}

% \listsource{../src}{main.cpp}
% \listsource{../src}{solver.hpp}
% \listsource{../src}{solver.cpp}
% \listsource{../src}{common.cpp}
% \listsource{../src}{exchange.hpp}
% \listsource{../src}{exchange.cpp}
% \listsource{../src}{problem.hpp}
% \listsource{../src}{problem.cpp}
% \listsource{../src}{grid/grid.hpp}
% \listsource{../src}{grid/grid.cpp}
% \listsource{../src}{dim3/dim3.hpp}
% \listsource{../src}{dim3/dim3.cpp}

\newpage

\se{Результаты}

\noindent
\textbf{ЛР №7}

\begin{table*}[!ht]
	\begin{minipage}{.49\linewidth}
		\centering
		\caption*{Область 8x8x8}
		точность $1e^{-10}$
		\begin{tabularx}{\linewidth}{|X|X|}
			\hline
			n processes & time      \\
			\hline
			1           & 5.053358  \\
			2           & 4.285806  \\
			4           & 5.089221  \\
			8           & 7.661748  \\
			16          & 33.624588 \\
			\hline
		\end{tabularx}
	\end{minipage}%
	\begin{minipage}{.49\linewidth}
		\centering
		\caption*{Область 64x64x64}
		точность $1e^{-2}$
		\begin{tabularx}{\linewidth}{|X|X|}
			\hline
			n processes & time        \\
			\hline
			1           & 1839.555659 \\
			2           & 945.581431  \\
			4           & 463.531061  \\
			8           & 270.607518  \\
			16          & 252.518910  \\
			\hline
		\end{tabularx}
	\end{minipage}

	\begin{minipage}{.49\linewidth}
		\centering
		\caption*{Область 128x128x128}
		точность $1e^{-2}$
		\begin{tabularx}{\linewidth}{|X|X|}
			\hline
			n processes & time         \\
			\hline
			1           & 15037.712376 \\
			2           & 7891.283137  \\
			4           & 3890.322194  \\
			8           & 2298.152495  \\
			16          & 1980.277726  \\
			\hline
		\end{tabularx}
	\end{minipage}%
\end{table*}

По первой таблице видно, что на большем количестве потоков результат стал хуже,
так как больше времени заняла передача граничных условий, чем сами вычисления.

На большем количестве данных скорость вычисления растёт с повышением числа процессов.

\textbf{ЛР №8}

При работе с CUDA получается не очень эффективно делать маленькие задачи и много процессов,
так как подготовка к вычислениям занимает намного больше времени.

Но есть смысл подобрать параметры запуска ядра для различных данных

\begin{table*}[!ht]
	\centering
	\caption*{Параметры запуска и время выполнения}
	точность $1e^{-10}$
	\begin{tabularx}{\linewidth}{|X|X|X|}
		\hline
		grid & block & time \\
		\hline
		% 1 &8& 
		\hline
	\end{tabularx}
\end{table*}


\textit{Область 8x8x8}:
\textit{Область 64x64x64}:
\textit{Область 128x128x128}:

\textbf{ЛР №9}

\begin{table*}[!ht]
	\begin{minipage}{.49\linewidth}
		\centering
		\caption*{Область 8x8x8}
		точность $1e^{-10}$
		\begin{tabularx}{\linewidth}{|X|X|}
			\hline
			n processes & time     \\
			\hline
			1           & 5.00552  \\
			2           & 4.17633  \\
			4           & 12685.4  \\
			8           & 27266.4  \\
			16          & 362840.0 \\
			\hline
		\end{tabularx}
	\end{minipage}%
	\begin{minipage}{.49\linewidth}
		\centering
		\caption*{Область 64x64x64}
		точность $1e^{-2}$
		\begin{tabularx}{\linewidth}{|X|X|}
			\hline
			n processes & time    \\
			\hline
			1           & 1348.7  \\
			2           & 688.18  \\
			4           & 11077.1 \\
			8           & 23388.6 \\
			16          & 57791.5 \\
			\hline
		\end{tabularx}
	\end{minipage}

	\centering

	\begin{minipage}{.49\linewidth}
		\centering
		\caption*{Область 128x128x128}
		точность $1e^{-2}$
		\begin{tabularx}{\linewidth}{|X|X|}
			\hline
			n processes & time    \\
			\hline
			1           & 11145.6 \\
			2           & 5782.32 \\
			4           & 12381.1 \\
			8           & 24338.2 \\
			16          & 58859.8 \\
			\hline
		\end{tabularx}
	\end{minipage}%
\end{table*}

Тут у всех задач примерно одинаковая динамика - прекрасные результаты для двух процессов и резкое падение
эффективности на большем количестве процессов. Я полагаю это связано с тем, что OpenMP создаёт несколько тредов
для каждого из процессов, что замедляет вычисления.

Если сравнивать с лучшими результатами ЛР №7, то на малых данных разница незаметна,
на средних и больших данных значительное отставание у OpenMP.

Скорее всего OpenMP мог бы показать себя лучше если бы у меня было больше ядер процессора.

\se{Выводы}

\end{document}